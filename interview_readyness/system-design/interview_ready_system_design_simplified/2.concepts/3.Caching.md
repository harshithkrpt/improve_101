# âš¡ï¸ Caching in Distributed Systems

Caching is a technique used to store frequently accessed data in a fast-access layer to reduce latency, avoid expensive operations, and improve system performance.

---

## âœ… Benefits of a Cache

- **ğŸš€ Saves Network Calls**  
  Frequently accessed remote data can be stored locally to reduce latency.

- **ğŸ” Avoids Repeated Computations**  
  Computation-heavy results (e.g., rendered templates, complex queries) can be reused.

- **ğŸ—ƒï¸ Reduces Database Load**  
  Relieves backend databases by answering common queries from the cache.

---

## âŒ Drawbacks of a Cache

- **ğŸ’° Can Be Expensive**  
  High-performance memory like RAM or SSDs used in caching increases cost.

- **ğŸ”„ Potential Thrashing**  
  If the cache size is small or the eviction policy is suboptimal, data might be repeatedly evicted and reloaded.

- **Eventual Consistency**
  That is based on the policy of the write the consistency is varied and can have stale data.

---

## ğŸ“œ Cache Eviction Policies

### 1. **LRU (Least Recently Used)**
- Removes the **least recently accessed** item when the cache is full.
- Assumes that recently used items are likely to be used again soon.

### 2. **LFU (Least Frequently Used)**
- Removes the **least frequently accessed** item.
- Better for scenarios where access frequency is a better predictor than recency.

---

## ğŸ§  Additional Concepts

- **TTL (Time to Live)**: Automatically expires cache entries after a time.
- **Write-Through Cache**: Writes to both cache and DB simultaneously.
- **Write-Back Cache**: Writes only to cache and pushes to DB later (asynchronously).
- **Read-Through Cache**: Automatically fetches from DB if cache miss occurs.
- **Cache Invalidation**: Mechanism to remove or update stale data.

---

## ğŸ§¾ Summary

| Aspect | Benefit |
|--------|---------|
| **Performance** | Reduces latency and improves response times |
| **Efficiency** | Lowers database load and resource usage |
| **Scalability** | Handles frequent reads efficiently |

Use caching wisely depending on access patterns, data volatility, and system constraints.

# ğŸ—‚ï¸ Cache Policy vs Eviction Policy

In caching systems, **cache policies** define how data is managed, and **eviction policies** determine which data to remove when the cache is full.

---

## ğŸ“¦ Cache Policy

Cache policy defines **how and when** data is loaded into and removed from the cache.

### Common Cache Strategies:

- **Read-Through Cache**  
  On a cache miss, the system reads from the database and populates the cache automatically.

- **Write-Through Cache**  
  Writes go to both the cache and the backing store (database) at the same time.

- **Write-Back (Write-Behind) Cache**  
  Writes go to the cache first, and the database is updated asynchronously later.

- **Refresh-Ahead Cache**  
  Periodically refreshes cache entries before they expire to avoid cache misses.

- **Cache Aside (Lazy Loading)**  
  The application code manually loads data into the cache on a miss and updates the cache after write operations.

---

## ğŸ§¹ Eviction Policy

Eviction policy determines **which item to remove** from the cache when it exceeds its memory/storage limit.

### Common Eviction Strategies:

- **LRU (Least Recently Used)**  
  Evicts the item that was accessed least recently.

- **LFU (Least Frequently Used)**  
  Evicts the item accessed the fewest number of times.

- **FIFO (First In, First Out)**  
  Evicts the oldest item i nserted into the cache.

- **MRU (Most Recently Used)**  
  Evicts the most recently used item (used in specific scenarios like stack-based caching).

- **Random Replacement**  
  Randomly evicts any item â€” simple but unpredictable.

---

## ğŸ§  Summary

| Policy Type       | Description                              |
|-------------------|------------------------------------------|
| **Cache Policy**  | Defines how data is added/updated in cache |
| **Eviction Policy** | Defines how data is removed when full    |

Choose cache & eviction policies based on your use case (read-heavy vs write-heavy, time sensitivity, etc.).


# ğŸ’¾ Write-Back Cache Policies

**Write-Back Cache** (also known as **Write-Behind**) is a caching strategy where data is first written to the cache, and the write to the underlying data store (e.g., database) happens **asynchronously**.

This improves write performance but introduces complexity in consistency and durability.

---

## ğŸ” Write-Back Policy Types

### 1. â±ï¸ TTL-Based (Time-To-Live Based)

- Data is written back to the main store **after a fixed time interval** (e.g., every 10 seconds).
- Useful for batching writes.
- Can reduce DB load but might lose data on crashes if not persisted.

**Example:**  
```text
Write to DB every 30 seconds after data is cached.
```

### 2. âš¡ Event-Based

- The write-back is triggered by specific **events** such as:
  - Cache eviction
  - User-defined events (e.g., save, logout)
  - Scheduled flush events

- Offers more control and avoids stale data on certain operations.

**Example:**  
```text
Flush cache to DB on user logout or session end.
```

### 3. ğŸ”„ Replacement-Based

- Data is written back **only when it is evicted** from the cache (due to LRU, LFU, etc.).
- Efficient but **risky**: if cache crashes before eviction, data may be lost.

**Example:**  
```text
Evict least recently used item and write it to DB during eviction.
```

---

## âš ï¸ Trade-Offs of Write-Back Caching

| Pros                           | Cons                             |
|--------------------------------|----------------------------------|
| Fast write performance         | Risk of data loss on crash       |
| Reduces database write load    | Harder to implement correctly    |
| Enables write batching         | Data is not immediately durable  |

---

## ğŸ§  Summary

| Policy Type        | Trigger                            | Use Case                              |
|--------------------|-------------------------------------|----------------------------------------|
| **TTL-Based**      | Time expiration                     | Periodic sync, non-critical writes     |
| **Event-Based**    | App or system event                 | Fine-grained control, user-driven sync |
| **Replacement-Based** | Eviction from cache             | Memory-constrained systems             |

Choose based on durability needs, write frequency, and system tolerance for delayed persistence.

# ğŸ“¦ Cache Eviction Policies: LRU, LFU, and Segmented LRU

Eviction policies determine **which item to remove** from the cache when it's full. The goal is to retain the most valuable data and evict the least useful.

---

## ğŸ•’ 1. LRU (Least Recently Used)

### ğŸ“Œ Definition:
Evicts the **least recently accessed** item when the cache is full.

### âœ… Pros:
- Simple to implement.
- Good for workloads with **temporal locality** (recently used data is likely to be used again).

### âŒ Cons:
- May evict frequently used items if not accessed recently.
- Not ideal for frequency-heavy access patterns.

### ğŸ§  Use Cases:
- General-purpose caches
- Browser cache
- In-memory stores like Redis, Memcached

---

## ğŸ”¢ 2. LFU (Least Frequently Used)

### ğŸ“Œ Definition:
Evicts the item that has been **accessed the fewest times**.

### âœ… Pros:
- Ideal for workloads where **some items are accessed much more frequently** than others.
- Retains hot data even if not recently used.

### âŒ Cons:
- Requires additional storage for frequency counters.
- May retain stale but frequently-used items (cold but hot once).
- Susceptible to **cache pollution** from short bursts of access.

### ğŸ§  Use Cases:
- Recommendation engines
- AI/ML workloads
- Long-lived caches

---

## ğŸ§ª 3. SLRU (Segmented LRU)

### ğŸ“Œ Definition:
**Segmented LRU** divides the cache into two or more segments:
- **Probationary segment**: Holds newly added items.
- **Protected segment**: Holds frequently reused items.

When an item is accessed again, it is promoted from probationary to protected.

### âœ… Pros:
- Balances **recency and frequency**.
- Prevents frequent eviction of recently added but unused items.
- Prevents cache pollution from one-time-use data.

### âŒ Cons:
- More complex to implement.
- Requires careful tuning of segment sizes.

### ğŸ§  Use Cases:
- OS page replacement algorithms
- Advanced caching systems (e.g., databases, web proxies)
- Level 1 and Level 2 CPU caches

---

## ğŸ“Š Comparison Table

| Policy | Based On | Pros | Cons | Best For |
|--------|----------|------|------|----------|
| **LRU** | Recency | Simple, efficient | May evict frequently used items | General caches |
| **LFU** | Frequency | Retains hot items | More memory + counter overhead | Hotspot-heavy apps |
| **SLRU** | Recency + Frequency | Balanced, avoids pollution | Complex | Advanced systems |

---

## ğŸ§  Summary

- **LRU**: Use when recent access is the best predictor.
- **LFU**: Use when frequent access is more important than recency.
- **SLRU**: Use when you want a hybrid that prevents both cold eviction and cache pollution.

