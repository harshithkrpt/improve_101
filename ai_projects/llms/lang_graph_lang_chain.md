### LangChain ðŸ”—: The LEGO Kit for LLMs

- **What it is:** A framework that provides modular building blocks (components) and standardized ways to connect them (chains).
- **Analogy:** Like a LEGO kit with organized pieces and instructions, it helps you build LLM applications quickly by handling the boilerplate code.
- **Core Idea:** Simplifies creating sequential applications where one step follows another.

### LangGraph ðŸ“ˆ: The Advanced LEGO Mindstorms

- **What it is:** A library for building complex, stateful, and often cyclical applications with LLMs.
- **Analogy:** Like LEGO Technic or Mindstorms, it allows for more intricate designs with loops, branches, and interacting parts.
- **Core Idea:** Perfect for multi-agent systems where different AI "agents" collaborate, reflect, and refine work in a non-linear way.

### How LangChain/LangGraph Differ from Standard LLM Frameworks

-   **Standard Libraries (e.g., `openai`):**
    -   Provide direct, low-level access to an LLM.
    -   You send a prompt, you get a text response.
    -   The developer is responsible for all surrounding logic: managing history, using tools, parsing output, and handling multi-step flows.

-   **LangChain ðŸ”—: The Universal Adapter & Toolkit**
    -   **Abstraction & Composability (LCEL):** Provides a "pipe" (`|`) syntax to easily connect components (`prompt | llm | parser`), reducing boilerplate code.
    -   **Standardized Interfaces:** Allows you to swap out components (like LLMs, e.g., OpenAI for Google) with minimal code changes. This is the key to flexibility and avoiding vendor lock-in.
    -   **Rich Ecosystem:** Comes "batteries-included" with hundreds of pre-built integrations for data loaders (PDF, SQL), tools (Search, Calculator), and more.

-   **LangGraph ðŸ“ˆ: The Control Flow Engine**
    -   **Explicit Logic:** Instead of complex `if/else` code, you define application flow as a graph with nodes (steps) and edges (paths).
    -   **State Management:** Specifically designed to manage stateful applications where memory and cycles (loops) are essential, perfect for complex agents.


```python
# Import the necessary components
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. Define the parts of our chain that WON'T change
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
output_parser = StrOutputParser()

# 2. Initialize the specific models you want to use
llm_openai = ChatOpenAI(model="gpt-4o")
llm_google = ChatGoogleGenerativeAI(model="gemini-1.5-pro")

# 3. Build a chain with OpenAI
chain_openai = prompt | llm_openai | output_parser

# 4. Build the EXACT SAME chain with Google's model
#    Notice only the `llm` variable is different!
chain_google = prompt | llm_google | output_parser

# Now you can run them both and compare!
print("From OpenAI:", chain_openai.invoke({"topic": "a robot"}))
print("From Google:", chain_google.invoke({"topic": "a robot"}))
```

### 1. Python
- **Requirement:** Python 3.9 or newer.
- **Check version:** `python3 --version`

### 2. Virtual Environment (`venv`)
- **Purpose:** Isolates project dependencies to prevent version conflicts.
- **Create:** `python3 -m venv .venv`
- **Activate (macOS/Linux):** `source .venv/bin/activate`
- **Activate (Windows):** `.\.venv\Scripts\activate`

### 3. Core Libraries (`pip`)
- **Run these commands inside your active virtual environment.**
- `pip install --upgrade pip`
- `pip install langchain langgraph`
- `pip install langchain-openai langchain-google-genai`
- `pip install python-dotenv`